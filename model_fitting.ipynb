{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:15:39.103999: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-29 14:15:39.185906: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-29 14:15:39.560415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 14:15:40.871237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all data as one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wavelength</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>Gain</th>\n",
       "      <th>Out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594.5</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>594.5</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>594.5</td>\n",
       "      <td>0.269444</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>594.5</td>\n",
       "      <td>0.379167</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>594.5</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>543.1</td>\n",
       "      <td>3.816279</td>\n",
       "      <td>0.233721</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>543.1</td>\n",
       "      <td>3.862209</td>\n",
       "      <td>0.187791</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>543.1</td>\n",
       "      <td>3.908140</td>\n",
       "      <td>0.141860</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>543.1</td>\n",
       "      <td>3.954070</td>\n",
       "      <td>0.095930</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>543.1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1192 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Wavelength        V1        V2  Gain       Out\n",
       "0          594.5  0.050000  0.050000     1 -0.000595\n",
       "1          594.5  0.159722  0.050000     1 -0.000549\n",
       "2          594.5  0.269444  0.050000     1 -0.000527\n",
       "3          594.5  0.379167  0.050000     1 -0.000454\n",
       "4          594.5  0.488889  0.050000     1 -0.000447\n",
       "...          ...       ...       ...   ...       ...\n",
       "1187       543.1  3.816279  0.233721     1  0.000996\n",
       "1188       543.1  3.862209  0.187791     1  0.001017\n",
       "1189       543.1  3.908140  0.141860     1  0.001049\n",
       "1190       543.1  3.954070  0.095930     1  0.001094\n",
       "1191       543.1  4.000000  0.050000     1  0.001131\n",
       "\n",
       "[1192 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'training_data/trainingdata_v2/'\n",
    "all_data = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "combined_data = pd.concat([pd.read_csv(file) for file in all_data], ignore_index=True)\n",
    "combined_data.drop([\"Unnamed: 0\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusts output voltage for the gain factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.loc[0,'Out']\n",
    "new_out = []\n",
    "for i in range(len(combined_data)):\n",
    "    adj_out = combined_data.loc[i,'Out']*combined_data.loc[i,'Gain']\n",
    "    new_out.append(adj_out)\n",
    "combined_data['Out'] = new_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting model as shown in svm_regression_test.ipynb (will go into further detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = combined_data['V1'].to_numpy()\n",
    "y = combined_data['V2'].to_numpy()\n",
    "z = combined_data['Gain Volts'].to_numpy(dtype = 'object')\n",
    "\n",
    "X = np.column_stack((x, y))\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(SVR(kernel='rbf'), param_grid, cv=5)\n",
    "grid_search.fit(X, z)\n",
    "best_c = grid_search.best_params_['C']\n",
    "best_gamma = grid_search.best_params_['gamma']\n",
    "\n",
    "model = SVR(kernel='rbf', C=best_c, gamma=best_gamma) \n",
    "model.fit(X,z)\n",
    "x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 \n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 500),\n",
    "                       np.linspace(x2_min, x2_max, 500))\n",
    "xx_input = np.column_stack((xx1.ravel(), xx2.ravel()))\n",
    "y_pred = model.predict(xx_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting Neural Network Fitting\n",
    "The above fit is a bit rough, and part of the problem is that it's wavelength agnostic. Of course, we do have a wavelength dependency in our data. However, the reason it's wavelength agnostic is because (from what I can gather) getting the predictions we'd like from a 4D SVM regressor is difficult. The current goal is to give two inputs (wavelength and desired polarization angle) and receive the input voltages needed for the LCVR's to produce the angle. Therefore a neural net may be more apt to fit this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:17:53.428567: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "## Test neural network code created with Gemini (modified slightly by me)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Converts dataframe to tensor\n",
    "# Note x is \"input\" wavelength, output voltage, Y is LCVR Volts\n",
    "x = combined_data.iloc[:, [1, 5]]\n",
    "y = combined_data.iloc[:, [2, 3]] \n",
    "X = tf.convert_to_tensor(x)\n",
    "Y = tf.convert_to_tensor(y)\n",
    "\n",
    "# Split into training and validation sets\n",
    "# Splitting ratio (e.g., 80% for training)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(split_ratio * len(X))\n",
    "\n",
    "# Splitting the tensors\n",
    "X_train = X[:split_index]\n",
    "X_val = X[split_index:]\n",
    "Y_train = Y[:split_index]\n",
    "Y_val = Y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(10, activation='relu', input_shape=(2,)),  # Hidden layer\n",
    "  layers.Dense(2)  # Output layer with 2 neurons\n",
    "])\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, Y_train, epochs=1000, validation_data=(X_val, Y_val), verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "[[2.6466823  0.46177444]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "new_input = tf.constant([[500,-.1]])  # Input with 2 known features\n",
    "prediction = model.predict(new_input)\n",
    "print(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first iteration. It's a bit better than SVM I believe but it could use some work. Now going to try tuning the model and such to better fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First scaling the data. Going to try the scaling layer included in Tensorflow.keras. Will scale based on a range from 0 to 1 for both which means we need the max values for each of our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmy/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 1s - 29ms/step - loss: 3.5568 - val_loss: 4.5585\n",
      "Epoch 2/300\n",
      "30/30 - 0s - 3ms/step - loss: 3.1740 - val_loss: 4.1254\n",
      "Epoch 3/300\n",
      "30/30 - 0s - 2ms/step - loss: 2.8661 - val_loss: 3.7544\n",
      "Epoch 4/300\n",
      "30/30 - 0s - 2ms/step - loss: 2.6006 - val_loss: 3.4365\n",
      "Epoch 5/300\n",
      "30/30 - 0s - 3ms/step - loss: 2.3837 - val_loss: 3.1541\n",
      "Epoch 6/300\n",
      "30/30 - 0s - 2ms/step - loss: 2.2084 - val_loss: 2.9087\n",
      "Epoch 7/300\n",
      "30/30 - 0s - 2ms/step - loss: 2.0739 - val_loss: 2.6978\n",
      "Epoch 8/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.9721 - val_loss: 2.5312\n",
      "Epoch 9/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.8965 - val_loss: 2.4009\n",
      "Epoch 10/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.8457 - val_loss: 2.2937\n",
      "Epoch 11/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.8122 - val_loss: 2.2090\n",
      "Epoch 12/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7856 - val_loss: 2.1365\n",
      "Epoch 13/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7623 - val_loss: 2.0870\n",
      "Epoch 14/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7495 - val_loss: 2.0395\n",
      "Epoch 15/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7348 - val_loss: 1.9967\n",
      "Epoch 16/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7279 - val_loss: 1.9707\n",
      "Epoch 17/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7202 - val_loss: 1.9504\n",
      "Epoch 18/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7185 - val_loss: 1.9281\n",
      "Epoch 19/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7143 - val_loss: 1.9145\n",
      "Epoch 20/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7134 - val_loss: 1.9073\n",
      "Epoch 21/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7085 - val_loss: 1.9021\n",
      "Epoch 22/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7092 - val_loss: 1.8905\n",
      "Epoch 23/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7101 - val_loss: 1.8946\n",
      "Epoch 24/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7078 - val_loss: 1.8804\n",
      "Epoch 25/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7079 - val_loss: 1.8869\n",
      "Epoch 26/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7063 - val_loss: 1.8833\n",
      "Epoch 27/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7027 - val_loss: 1.8833\n",
      "Epoch 28/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7011 - val_loss: 1.8815\n",
      "Epoch 29/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7013 - val_loss: 1.8844\n",
      "Epoch 30/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.7047 - val_loss: 1.8901\n",
      "Epoch 31/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6991 - val_loss: 1.8832\n",
      "Epoch 32/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6993 - val_loss: 1.8768\n",
      "Epoch 33/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6957 - val_loss: 1.8826\n",
      "Epoch 34/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6948 - val_loss: 1.8817\n",
      "Epoch 35/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6960 - val_loss: 1.8819\n",
      "Epoch 36/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6917 - val_loss: 1.8816\n",
      "Epoch 37/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6914 - val_loss: 1.8910\n",
      "Epoch 38/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6925 - val_loss: 1.8800\n",
      "Epoch 39/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6876 - val_loss: 1.8899\n",
      "Epoch 40/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6875 - val_loss: 1.8856\n",
      "Epoch 41/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6833 - val_loss: 1.8873\n",
      "Epoch 42/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6860 - val_loss: 1.8934\n",
      "Epoch 43/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6807 - val_loss: 1.8871\n",
      "Epoch 44/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6806 - val_loss: 1.8900\n",
      "Epoch 45/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6790 - val_loss: 1.8872\n",
      "Epoch 46/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6821 - val_loss: 1.8986\n",
      "Epoch 47/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6804 - val_loss: 1.8875\n",
      "Epoch 48/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6750 - val_loss: 1.8974\n",
      "Epoch 49/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6774 - val_loss: 1.8959\n",
      "Epoch 50/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6789 - val_loss: 1.8952\n",
      "Epoch 51/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6730 - val_loss: 1.8989\n",
      "Epoch 52/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6733 - val_loss: 1.9021\n",
      "Epoch 53/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6713 - val_loss: 1.8912\n",
      "Epoch 54/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6700 - val_loss: 1.8987\n",
      "Epoch 55/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6675 - val_loss: 1.8936\n",
      "Epoch 56/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6687 - val_loss: 1.9100\n",
      "Epoch 57/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6691 - val_loss: 1.9006\n",
      "Epoch 58/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6649 - val_loss: 1.8927\n",
      "Epoch 59/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6650 - val_loss: 1.9083\n",
      "Epoch 60/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6658 - val_loss: 1.8891\n",
      "Epoch 61/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6649 - val_loss: 1.9025\n",
      "Epoch 62/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6640 - val_loss: 1.9057\n",
      "Epoch 63/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6635 - val_loss: 1.9209\n",
      "Epoch 64/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6608 - val_loss: 1.9053\n",
      "Epoch 65/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6583 - val_loss: 1.8928\n",
      "Epoch 66/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6599 - val_loss: 1.9070\n",
      "Epoch 67/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6585 - val_loss: 1.9111\n",
      "Epoch 68/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6582 - val_loss: 1.9152\n",
      "Epoch 69/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6572 - val_loss: 1.9096\n",
      "Epoch 70/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6550 - val_loss: 1.9090\n",
      "Epoch 71/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6572 - val_loss: 1.9111\n",
      "Epoch 72/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6560 - val_loss: 1.9097\n",
      "Epoch 73/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6562 - val_loss: 1.9049\n",
      "Epoch 74/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6505 - val_loss: 1.9212\n",
      "Epoch 75/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6558 - val_loss: 1.9235\n",
      "Epoch 76/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6516 - val_loss: 1.9144\n",
      "Epoch 77/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6530 - val_loss: 1.9194\n",
      "Epoch 78/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6486 - val_loss: 1.9077\n",
      "Epoch 79/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6499 - val_loss: 1.9162\n",
      "Epoch 80/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6477 - val_loss: 1.9120\n",
      "Epoch 81/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6500 - val_loss: 1.9349\n",
      "Epoch 82/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6518 - val_loss: 1.9341\n",
      "Epoch 83/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6478 - val_loss: 1.9145\n",
      "Epoch 84/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6457 - val_loss: 1.9269\n",
      "Epoch 85/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6477 - val_loss: 1.9254\n",
      "Epoch 86/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6470 - val_loss: 1.9203\n",
      "Epoch 87/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6459 - val_loss: 1.9294\n",
      "Epoch 88/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6453 - val_loss: 1.9305\n",
      "Epoch 89/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6447 - val_loss: 1.9217\n",
      "Epoch 90/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6440 - val_loss: 1.9332\n",
      "Epoch 91/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6458 - val_loss: 1.9309\n",
      "Epoch 92/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6432 - val_loss: 1.9202\n",
      "Epoch 93/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6435 - val_loss: 1.9344\n",
      "Epoch 94/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6443 - val_loss: 1.9331\n",
      "Epoch 95/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6407 - val_loss: 1.9375\n",
      "Epoch 96/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6474 - val_loss: 1.9368\n",
      "Epoch 97/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6424 - val_loss: 1.9370\n",
      "Epoch 98/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6426 - val_loss: 1.9289\n",
      "Epoch 99/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6413 - val_loss: 1.9414\n",
      "Epoch 100/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6407 - val_loss: 1.9371\n",
      "Epoch 101/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6404 - val_loss: 1.9424\n",
      "Epoch 102/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6437 - val_loss: 1.9683\n",
      "Epoch 103/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6375 - val_loss: 1.9417\n",
      "Epoch 104/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6391 - val_loss: 1.9408\n",
      "Epoch 105/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6410 - val_loss: 1.9467\n",
      "Epoch 106/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6381 - val_loss: 1.9471\n",
      "Epoch 107/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6377 - val_loss: 1.9304\n",
      "Epoch 108/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6378 - val_loss: 1.9552\n",
      "Epoch 109/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6351 - val_loss: 1.9443\n",
      "Epoch 110/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6361 - val_loss: 1.9434\n",
      "Epoch 111/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6386 - val_loss: 1.9580\n",
      "Epoch 112/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6390 - val_loss: 1.9296\n",
      "Epoch 113/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6396 - val_loss: 1.9594\n",
      "Epoch 114/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6333 - val_loss: 1.9490\n",
      "Epoch 115/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6380 - val_loss: 1.9438\n",
      "Epoch 116/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6366 - val_loss: 1.9632\n",
      "Epoch 117/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6348 - val_loss: 1.9586\n",
      "Epoch 118/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6339 - val_loss: 1.9515\n",
      "Epoch 119/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6351 - val_loss: 1.9576\n",
      "Epoch 120/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6331 - val_loss: 1.9518\n",
      "Epoch 121/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6337 - val_loss: 1.9654\n",
      "Epoch 122/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6350 - val_loss: 1.9745\n",
      "Epoch 123/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6346 - val_loss: 1.9558\n",
      "Epoch 124/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6321 - val_loss: 1.9503\n",
      "Epoch 125/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6305 - val_loss: 1.9524\n",
      "Epoch 126/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6291 - val_loss: 1.9594\n",
      "Epoch 127/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6320 - val_loss: 1.9570\n",
      "Epoch 128/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6303 - val_loss: 1.9707\n",
      "Epoch 129/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6296 - val_loss: 1.9684\n",
      "Epoch 130/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6290 - val_loss: 1.9577\n",
      "Epoch 131/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6294 - val_loss: 1.9633\n",
      "Epoch 132/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6303 - val_loss: 1.9682\n",
      "Epoch 133/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6269 - val_loss: 1.9526\n",
      "Epoch 134/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6280 - val_loss: 1.9646\n",
      "Epoch 135/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6288 - val_loss: 1.9743\n",
      "Epoch 136/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6277 - val_loss: 1.9607\n",
      "Epoch 137/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6327 - val_loss: 1.9736\n",
      "Epoch 138/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6277 - val_loss: 1.9758\n",
      "Epoch 139/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6249 - val_loss: 1.9611\n",
      "Epoch 140/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6256 - val_loss: 1.9789\n",
      "Epoch 141/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6251 - val_loss: 1.9700\n",
      "Epoch 142/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6289 - val_loss: 1.9781\n",
      "Epoch 143/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6257 - val_loss: 1.9747\n",
      "Epoch 144/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6273 - val_loss: 1.9739\n",
      "Epoch 145/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6235 - val_loss: 1.9882\n",
      "Epoch 146/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6225 - val_loss: 1.9836\n",
      "Epoch 147/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6240 - val_loss: 1.9760\n",
      "Epoch 148/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6252 - val_loss: 1.9546\n",
      "Epoch 149/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6211 - val_loss: 1.9797\n",
      "Epoch 150/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6217 - val_loss: 1.9840\n",
      "Epoch 151/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6224 - val_loss: 1.9872\n",
      "Epoch 152/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6222 - val_loss: 1.9794\n",
      "Epoch 153/300\n",
      "30/30 - 0s - 3ms/step - loss: 1.6230 - val_loss: 1.9829\n",
      "Epoch 154/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6216 - val_loss: 1.9956\n",
      "Epoch 155/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6241 - val_loss: 1.9816\n",
      "Epoch 156/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6238 - val_loss: 1.9790\n",
      "Epoch 157/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6229 - val_loss: 2.0066\n",
      "Epoch 158/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6226 - val_loss: 2.0080\n",
      "Epoch 159/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6208 - val_loss: 1.9917\n",
      "Epoch 160/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6202 - val_loss: 1.9646\n",
      "Epoch 161/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6155 - val_loss: 2.0128\n",
      "Epoch 162/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6204 - val_loss: 2.0026\n",
      "Epoch 163/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6197 - val_loss: 1.9852\n",
      "Epoch 164/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6191 - val_loss: 1.9980\n",
      "Epoch 165/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6211 - val_loss: 1.9786\n",
      "Epoch 166/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.9958\n",
      "Epoch 167/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6203 - val_loss: 2.0022\n",
      "Epoch 168/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6182 - val_loss: 1.9854\n",
      "Epoch 169/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6146 - val_loss: 1.9992\n",
      "Epoch 170/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.9903\n",
      "Epoch 171/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6208 - val_loss: 2.0034\n",
      "Epoch 172/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6145 - val_loss: 1.9881\n",
      "Epoch 173/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6164 - val_loss: 2.0074\n",
      "Epoch 174/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.9695\n",
      "Epoch 175/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6175 - val_loss: 2.0150\n",
      "Epoch 176/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6181 - val_loss: 1.9770\n",
      "Epoch 177/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6127 - val_loss: 2.0240\n",
      "Epoch 178/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6163 - val_loss: 2.0291\n",
      "Epoch 179/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6163 - val_loss: 1.9888\n",
      "Epoch 180/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6184 - val_loss: 1.9992\n",
      "Epoch 181/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6164 - val_loss: 2.0199\n",
      "Epoch 182/300\n",
      "30/30 - 0s - 2ms/step - loss: 1.6120 - val_loss: 2.0009\n",
      "Epoch 183/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      7\u001b[0m   Rescaling(scale\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mmax_wavelength, \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39moutput_range], offset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, min_output]),\n\u001b[1;32m      8\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,)),  \u001b[38;5;66;03m# Hidden layer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Output layer with 2 neurons\u001b[39;00m\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:138\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_type.py:391\u001b[0m, in \u001b[0;36mFunctionType.unpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    388\u001b[0m flat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sorted_parameters:\n\u001b[1;32m    390\u001b[0m   flat\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 391\u001b[0m       \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m   )\n\u001b[1;32m    394\u001b[0m dealiased_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    395\u001b[0m ids_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:253\u001b[0m, in \u001b[0;36mTypeSpec.to_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See TraceType base class for details. Do not override.\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    251\u001b[0m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m spec, v: tensors\u001b[38;5;241m.\u001b[39mextend(spec\u001b[38;5;241m.\u001b[39mto_tensors(v)),\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_component_specs\u001b[49m,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_components(value))\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:948\u001b[0m, in \u001b[0;36mIteratorSpec._component_specs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_component_specs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 948\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource\u001b[49m\u001b[43m)\u001b[49m,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:865\u001b[0m, in \u001b[0;36mDenseSpec.__init__\u001b[0;34m(self, shape, dtype, name)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mfloat32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a TensorSpec.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03m      not convertible to a `tf.DType`.\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorShape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_wavelength = x['Wavelength'].max()\n",
    "min_output = x['Out'].min()\n",
    "max_output = x['Out'].max()\n",
    "output_range = max_output - min_output\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  Rescaling(scale=[1./max_wavelength, 1./output_range], offset=[0, min_output]),\n",
    "  layers.Dense(10, activation='relu', input_shape=(2,)),  # Hidden layer\n",
    "  layers.Dense(2)  # Output layer with 2 neurons\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, Y_train, epochs=300, validation_data=(X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat helpful, as we have lower RMS error in less time. Now will try adding ReLU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "35/35 - 1s - 27ms/step - loss: 3.8983 - val_loss: 4.5996\n",
      "Epoch 2/300\n",
      "35/35 - 0s - 2ms/step - loss: 3.6428 - val_loss: 4.3448\n",
      "Epoch 3/300\n",
      "35/35 - 0s - 2ms/step - loss: 3.4032 - val_loss: 4.0079\n",
      "Epoch 4/300\n",
      "35/35 - 0s - 2ms/step - loss: 3.0920 - val_loss: 3.5524\n",
      "Epoch 5/300\n",
      "35/35 - 0s - 2ms/step - loss: 2.7010 - val_loss: 3.0128\n",
      "Epoch 6/300\n",
      "35/35 - 0s - 2ms/step - loss: 2.2983 - val_loss: 2.4757\n",
      "Epoch 7/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.9639 - val_loss: 2.0556\n",
      "Epoch 8/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7760 - val_loss: 1.8476\n",
      "Epoch 9/300\n",
      "35/35 - 0s - 3ms/step - loss: 1.7307 - val_loss: 1.7846\n",
      "Epoch 10/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7247 - val_loss: 1.7780\n",
      "Epoch 11/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7223 - val_loss: 1.7763\n",
      "Epoch 12/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7201 - val_loss: 1.7689\n",
      "Epoch 13/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7175 - val_loss: 1.7754\n",
      "Epoch 14/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7150 - val_loss: 1.7651\n",
      "Epoch 15/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7116 - val_loss: 1.7610\n",
      "Epoch 16/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7089 - val_loss: 1.7562\n",
      "Epoch 17/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7056 - val_loss: 1.7554\n",
      "Epoch 18/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.7031 - val_loss: 1.7495\n",
      "Epoch 19/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6988 - val_loss: 1.7438\n",
      "Epoch 20/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6967 - val_loss: 1.7457\n",
      "Epoch 21/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6921 - val_loss: 1.7437\n",
      "Epoch 22/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6899 - val_loss: 1.7381\n",
      "Epoch 23/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6846 - val_loss: 1.7250\n",
      "Epoch 24/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6814 - val_loss: 1.7288\n",
      "Epoch 25/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6771 - val_loss: 1.7276\n",
      "Epoch 26/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6749 - val_loss: 1.7148\n",
      "Epoch 27/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6718 - val_loss: 1.7125\n",
      "Epoch 28/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6676 - val_loss: 1.7097\n",
      "Epoch 29/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6650 - val_loss: 1.7034\n",
      "Epoch 30/300\n",
      "35/35 - 0s - 3ms/step - loss: 1.6615 - val_loss: 1.7002\n",
      "Epoch 31/300\n",
      "35/35 - 0s - 3ms/step - loss: 1.6594 - val_loss: 1.7025\n",
      "Epoch 32/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6581 - val_loss: 1.6983\n",
      "Epoch 33/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6553 - val_loss: 1.6930\n",
      "Epoch 34/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6558 - val_loss: 1.6944\n",
      "Epoch 35/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6537 - val_loss: 1.6855\n",
      "Epoch 36/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6510 - val_loss: 1.6865\n",
      "Epoch 37/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6495 - val_loss: 1.6816\n",
      "Epoch 38/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6502 - val_loss: 1.6731\n",
      "Epoch 39/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6499 - val_loss: 1.6792\n",
      "Epoch 40/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6480 - val_loss: 1.6802\n",
      "Epoch 41/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6468 - val_loss: 1.6772\n",
      "Epoch 42/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6459 - val_loss: 1.6809\n",
      "Epoch 43/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6457 - val_loss: 1.6870\n",
      "Epoch 44/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6428 - val_loss: 1.6974\n",
      "Epoch 45/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6366 - val_loss: 1.7159\n",
      "Epoch 46/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6333 - val_loss: 1.7062\n",
      "Epoch 47/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6325 - val_loss: 1.7089\n",
      "Epoch 48/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6284 - val_loss: 1.6994\n",
      "Epoch 49/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6270 - val_loss: 1.7022\n",
      "Epoch 50/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6259 - val_loss: 1.7014\n",
      "Epoch 51/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6247 - val_loss: 1.6944\n",
      "Epoch 52/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6242 - val_loss: 1.6946\n",
      "Epoch 53/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6230 - val_loss: 1.6972\n",
      "Epoch 54/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6225 - val_loss: 1.7007\n",
      "Epoch 55/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6220 - val_loss: 1.6957\n",
      "Epoch 56/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6218 - val_loss: 1.6980\n",
      "Epoch 57/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6225 - val_loss: 1.6978\n",
      "Epoch 58/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6214 - val_loss: 1.6872\n",
      "Epoch 59/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6214 - val_loss: 1.6935\n",
      "Epoch 60/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6225 - val_loss: 1.6896\n",
      "Epoch 61/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6214 - val_loss: 1.6925\n",
      "Epoch 62/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6208 - val_loss: 1.6947\n",
      "Epoch 63/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6933\n",
      "Epoch 64/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6208 - val_loss: 1.6912\n",
      "Epoch 65/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6962\n",
      "Epoch 66/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6210 - val_loss: 1.6970\n",
      "Epoch 67/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6212 - val_loss: 1.6909\n",
      "Epoch 68/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6213 - val_loss: 1.6943\n",
      "Epoch 69/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6221 - val_loss: 1.7024\n",
      "Epoch 70/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6969\n",
      "Epoch 71/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6933\n",
      "Epoch 72/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6206 - val_loss: 1.6910\n",
      "Epoch 73/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6971\n",
      "Epoch 74/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6220 - val_loss: 1.6877\n",
      "Epoch 75/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6208 - val_loss: 1.6975\n",
      "Epoch 76/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6218 - val_loss: 1.6934\n",
      "Epoch 77/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6215 - val_loss: 1.6932\n",
      "Epoch 78/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6213 - val_loss: 1.6892\n",
      "Epoch 79/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6968\n",
      "Epoch 80/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6214 - val_loss: 1.6988\n",
      "Epoch 81/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6208 - val_loss: 1.6960\n",
      "Epoch 82/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6909\n",
      "Epoch 83/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6967\n",
      "Epoch 84/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6884\n",
      "Epoch 85/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6211 - val_loss: 1.6878\n",
      "Epoch 86/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6205 - val_loss: 1.6917\n",
      "Epoch 87/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6206 - val_loss: 1.6893\n",
      "Epoch 88/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6207 - val_loss: 1.6978\n",
      "Epoch 89/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6212 - val_loss: 1.6981\n",
      "Epoch 90/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6202 - val_loss: 1.6900\n",
      "Epoch 91/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6205 - val_loss: 1.6908\n",
      "Epoch 92/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6936\n",
      "Epoch 93/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6902\n",
      "Epoch 94/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6203 - val_loss: 1.6953\n",
      "Epoch 95/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6955\n",
      "Epoch 96/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6207 - val_loss: 1.6961\n",
      "Epoch 97/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6891\n",
      "Epoch 98/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6910\n",
      "Epoch 99/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6923\n",
      "Epoch 100/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6916\n",
      "Epoch 101/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6203 - val_loss: 1.6922\n",
      "Epoch 102/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6206 - val_loss: 1.7009\n",
      "Epoch 103/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6902\n",
      "Epoch 104/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6875\n",
      "Epoch 105/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6918\n",
      "Epoch 106/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6206 - val_loss: 1.6968\n",
      "Epoch 107/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6915\n",
      "Epoch 108/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6206 - val_loss: 1.6927\n",
      "Epoch 109/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6936\n",
      "Epoch 110/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6945\n",
      "Epoch 111/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6218 - val_loss: 1.6942\n",
      "Epoch 112/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6221 - val_loss: 1.6897\n",
      "Epoch 113/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6910\n",
      "Epoch 114/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6952\n",
      "Epoch 115/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6211 - val_loss: 1.6881\n",
      "Epoch 116/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6908\n",
      "Epoch 117/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6202 - val_loss: 1.6883\n",
      "Epoch 118/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6205 - val_loss: 1.6878\n",
      "Epoch 119/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6213 - val_loss: 1.6924\n",
      "Epoch 120/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6211 - val_loss: 1.6928\n",
      "Epoch 121/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6213 - val_loss: 1.6879\n",
      "Epoch 122/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6921\n",
      "Epoch 123/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6223 - val_loss: 1.6945\n",
      "Epoch 124/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6203 - val_loss: 1.6860\n",
      "Epoch 125/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6203 - val_loss: 1.6893\n",
      "Epoch 126/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6207 - val_loss: 1.6868\n",
      "Epoch 127/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6944\n",
      "Epoch 128/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6919\n",
      "Epoch 129/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6884\n",
      "Epoch 130/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6897\n",
      "Epoch 131/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6908\n",
      "Epoch 132/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6210 - val_loss: 1.6889\n",
      "Epoch 133/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6205 - val_loss: 1.6920\n",
      "Epoch 134/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6920\n",
      "Epoch 135/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6909\n",
      "Epoch 136/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6871\n",
      "Epoch 137/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6194 - val_loss: 1.6923\n",
      "Epoch 138/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6197 - val_loss: 1.6917\n",
      "Epoch 139/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6209 - val_loss: 1.6921\n",
      "Epoch 140/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6221 - val_loss: 1.6918\n",
      "Epoch 141/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6203 - val_loss: 1.6893\n",
      "Epoch 142/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6953\n",
      "Epoch 143/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6918\n",
      "Epoch 144/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6916\n",
      "Epoch 145/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6835\n",
      "Epoch 146/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6939\n",
      "Epoch 147/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6868\n",
      "Epoch 148/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6192 - val_loss: 1.6924\n",
      "Epoch 149/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6941\n",
      "Epoch 150/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6933\n",
      "Epoch 151/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6194 - val_loss: 1.6872\n",
      "Epoch 152/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6925\n",
      "Epoch 153/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6197 - val_loss: 1.6904\n",
      "Epoch 154/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6205 - val_loss: 1.6887\n",
      "Epoch 155/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6192 - val_loss: 1.6923\n",
      "Epoch 156/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6204 - val_loss: 1.6939\n",
      "Epoch 157/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6192 - val_loss: 1.6897\n",
      "Epoch 158/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6921\n",
      "Epoch 159/300\n",
      "35/35 - 0s - 3ms/step - loss: 1.6196 - val_loss: 1.6914\n",
      "Epoch 160/300\n",
      "35/35 - 0s - 3ms/step - loss: 1.6192 - val_loss: 1.6879\n",
      "Epoch 161/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6197 - val_loss: 1.6992\n",
      "Epoch 162/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6896\n",
      "Epoch 163/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6914\n",
      "Epoch 164/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6202 - val_loss: 1.6946\n",
      "Epoch 165/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6190 - val_loss: 1.6847\n",
      "Epoch 166/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6903\n",
      "Epoch 167/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6215 - val_loss: 1.6936\n",
      "Epoch 168/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6197 - val_loss: 1.6810\n",
      "Epoch 169/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6909\n",
      "Epoch 170/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6915\n",
      "Epoch 171/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6201 - val_loss: 1.6907\n",
      "Epoch 172/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6206 - val_loss: 1.6804\n",
      "Epoch 173/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6202 - val_loss: 1.6913\n",
      "Epoch 174/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6893\n",
      "Epoch 175/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6205 - val_loss: 1.6942\n",
      "Epoch 176/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6194 - val_loss: 1.6897\n",
      "Epoch 177/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6865\n",
      "Epoch 178/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6901\n",
      "Epoch 179/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6200 - val_loss: 1.6880\n",
      "Epoch 180/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6194 - val_loss: 1.6974\n",
      "Epoch 181/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6844\n",
      "Epoch 182/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6988\n",
      "Epoch 183/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6904\n",
      "Epoch 184/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6927\n",
      "Epoch 185/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6191 - val_loss: 1.6931\n",
      "Epoch 186/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6875\n",
      "Epoch 187/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6196 - val_loss: 1.6922\n",
      "Epoch 188/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6890\n",
      "Epoch 189/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6192 - val_loss: 1.6908\n",
      "Epoch 190/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6186 - val_loss: 1.6951\n",
      "Epoch 191/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6198 - val_loss: 1.6949\n",
      "Epoch 192/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6187 - val_loss: 1.6905\n",
      "Epoch 193/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6920\n",
      "Epoch 194/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6934\n",
      "Epoch 195/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6212 - val_loss: 1.6888\n",
      "Epoch 196/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6203 - val_loss: 1.6873\n",
      "Epoch 197/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6186 - val_loss: 1.6890\n",
      "Epoch 198/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6189 - val_loss: 1.6898\n",
      "Epoch 199/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6199 - val_loss: 1.6983\n",
      "Epoch 200/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6190 - val_loss: 1.6904\n",
      "Epoch 201/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6927\n",
      "Epoch 202/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6190 - val_loss: 1.6920\n",
      "Epoch 203/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6188 - val_loss: 1.6910\n",
      "Epoch 204/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6191 - val_loss: 1.6914\n",
      "Epoch 205/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6188 - val_loss: 1.6903\n",
      "Epoch 206/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6192 - val_loss: 1.6867\n",
      "Epoch 207/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6857\n",
      "Epoch 208/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6982\n",
      "Epoch 209/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6193 - val_loss: 1.6914\n",
      "Epoch 210/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6191 - val_loss: 1.6872\n",
      "Epoch 211/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6192 - val_loss: 1.6876\n",
      "Epoch 212/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6190 - val_loss: 1.6866\n",
      "Epoch 213/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6195 - val_loss: 1.6886\n",
      "Epoch 214/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6191 - val_loss: 1.6884\n",
      "Epoch 215/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6184 - val_loss: 1.6892\n",
      "Epoch 216/300\n",
      "35/35 - 0s - 2ms/step - loss: 1.6188 - val_loss: 1.6892\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3063\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3063\u001b[0m     arg_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   3065\u001b[0m     \u001b[38;5;66;03m# No more positional arguments\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      7\u001b[0m   Rescaling(scale\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mmax_wavelength, \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39moutput_range], offset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, min_output]),\n\u001b[1;32m      8\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,)),\n\u001b[1;32m      9\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,)),  \u001b[38;5;66;03m# Hidden layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Output layer with 2 neurons\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:137\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    132\u001b[0m function \u001b[38;5;241m=\u001b[39m trace_function(\n\u001b[1;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, tracing_options\u001b[38;5;241m=\u001b[39mtracing_options\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3063\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   3060\u001b[0m     \u001b[38;5;66;03m# Let's iterate through the positional arguments and corresponding\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m     \u001b[38;5;66;03m# parameters\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3063\u001b[0m         arg_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3064\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   3065\u001b[0m         \u001b[38;5;66;03m# No more positional arguments\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_wavelength = x['Wavelength'].max()\n",
    "min_output = x['Out'].min()\n",
    "max_output = x['Out'].max()\n",
    "output_range = max_output - min_output\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  Rescaling(scale=[1./max_wavelength, 1./output_range], offset=[0, min_output]),\n",
    "  layers.Dense(10, activation='relu', input_shape=(2,)),\n",
    "  layers.Dense(10, activation='relu', input_shape=(2,)),  # Hidden layer\n",
    "  layers.Dense(2)  # Output layer with 2 neurons\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, Y_train, epochs=300, validation_data=(X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even better and continually decreases the loss... Sometimes? I need to figure out how to make that more consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling input, target, and validation data. There should be an easier way but I wanted to have the scale factors stored as well for recovery later. (Maybe make this a function when fully implemented?)\n",
    "\n",
    "NOTE: does not currently store scale factors actually but it does scale properly for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#Y training\n",
    "rs_Y_train = tf.concat([\n",
    "    tf.convert_to_tensor(scaler.fit(Y_train[:,0:1]).transform(Y_train[:, 0:1])),\n",
    "    tf.convert_to_tensor(scaler.fit(Y_train[:,1:]).transform(Y_train[:, 1:]))\n",
    "], axis=1)\n",
    "\n",
    "# X training\n",
    "rs_X_train = tf.concat([\n",
    "    tf.convert_to_tensor(scaler.fit(X_train[:,0:1]).transform(X_train[:, 0:1])),\n",
    "    tf.convert_to_tensor(scaler.fit(X_train[:,1:]).transform(X_train[:, 1:]))\n",
    "], axis=1)\n",
    "\n",
    "#Y validation\n",
    "# Want same scaler on training and validation data\n",
    "rs_Y_val = tf.concat([\n",
    "    tf.convert_to_tensor(scaler.fit(Y_train[:,0:1]).transform(Y_val[:, 0:1])),\n",
    "    tf.convert_to_tensor(scaler.fit(Y_train[:,1:]).transform(Y_val[:, 1:]))\n",
    "], axis=1)\n",
    "\n",
    "#X validation\n",
    "rs_X_val = tf.concat([\n",
    "    tf.convert_to_tensor(scaler.fit(X_train[:,0:1]).transform(X_val[:, 0:1])),\n",
    "    tf.convert_to_tensor(scaler.fit(X_train[:,1:]).transform(X_val[:, 1:]))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmy/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 - 1s - 29ms/step - loss: 0.2742 - val_loss: 0.2308\n",
      "Epoch 2/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1651 - val_loss: 0.1698\n",
      "Epoch 3/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1390 - val_loss: 0.1429\n",
      "Epoch 4/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1371 - val_loss: 0.1321\n",
      "Epoch 5/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1304 - val_loss: 0.1294\n",
      "Epoch 6/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1250 - val_loss: 0.1274\n",
      "Epoch 7/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1222 - val_loss: 0.1263\n",
      "Epoch 8/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1215 - val_loss: 0.1251\n",
      "Epoch 9/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1199 - val_loss: 0.1244\n",
      "Epoch 10/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1167 - val_loss: 0.1236\n",
      "Epoch 11/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1165 - val_loss: 0.1228\n",
      "Epoch 12/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1136 - val_loss: 0.1216\n",
      "Epoch 13/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1158 - val_loss: 0.1212\n",
      "Epoch 14/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1152 - val_loss: 0.1208\n",
      "Epoch 15/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1126 - val_loss: 0.1198\n",
      "Epoch 16/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1134 - val_loss: 0.1197\n",
      "Epoch 17/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1122 - val_loss: 0.1187\n",
      "Epoch 18/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1136 - val_loss: 0.1187\n",
      "Epoch 19/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1130 - val_loss: 0.1177\n",
      "Epoch 20/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1122 - val_loss: 0.1171\n",
      "Epoch 21/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1120 - val_loss: 0.1168\n",
      "Epoch 22/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1113 - val_loss: 0.1168\n",
      "Epoch 23/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1124 - val_loss: 0.1156\n",
      "Epoch 24/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1118 - val_loss: 0.1154\n",
      "Epoch 25/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1127 - val_loss: 0.1156\n",
      "Epoch 26/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1115 - val_loss: 0.1159\n",
      "Epoch 27/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1111 - val_loss: 0.1148\n",
      "Epoch 28/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1115 - val_loss: 0.1153\n",
      "Epoch 29/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1099 - val_loss: 0.1142\n",
      "Epoch 30/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1103 - val_loss: 0.1140\n",
      "Epoch 31/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1103 - val_loss: 0.1137\n",
      "Epoch 32/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1102 - val_loss: 0.1136\n",
      "Epoch 33/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1086 - val_loss: 0.1135\n",
      "Epoch 34/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1089 - val_loss: 0.1134\n",
      "Epoch 35/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1097 - val_loss: 0.1134\n",
      "Epoch 36/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1094 - val_loss: 0.1120\n",
      "Epoch 37/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1092 - val_loss: 0.1126\n",
      "Epoch 38/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1112 - val_loss: 0.1129\n",
      "Epoch 39/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1077 - val_loss: 0.1125\n",
      "Epoch 40/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1081 - val_loss: 0.1124\n",
      "Epoch 41/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1091 - val_loss: 0.1123\n",
      "Epoch 42/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1090 - val_loss: 0.1121\n",
      "Epoch 43/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1079 - val_loss: 0.1114\n",
      "Epoch 44/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1087 - val_loss: 0.1117\n",
      "Epoch 45/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1085 - val_loss: 0.1109\n",
      "Epoch 46/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1086 - val_loss: 0.1118\n",
      "Epoch 47/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1078 - val_loss: 0.1116\n",
      "Epoch 48/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1089 - val_loss: 0.1112\n",
      "Epoch 49/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1079 - val_loss: 0.1106\n",
      "Epoch 50/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1083 - val_loss: 0.1118\n",
      "Epoch 51/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1073 - val_loss: 0.1105\n",
      "Epoch 52/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1077 - val_loss: 0.1111\n",
      "Epoch 53/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1070 - val_loss: 0.1110\n",
      "Epoch 54/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1077 - val_loss: 0.1109\n",
      "Epoch 55/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1077 - val_loss: 0.1102\n",
      "Epoch 56/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1075 - val_loss: 0.1104\n",
      "Epoch 57/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1074 - val_loss: 0.1099\n",
      "Epoch 58/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1075 - val_loss: 0.1105\n",
      "Epoch 59/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1069 - val_loss: 0.1096\n",
      "Epoch 60/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1074 - val_loss: 0.1108\n",
      "Epoch 61/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1072 - val_loss: 0.1102\n",
      "Epoch 62/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1070 - val_loss: 0.1095\n",
      "Epoch 63/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1078 - val_loss: 0.1100\n",
      "Epoch 64/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1072 - val_loss: 0.1100\n",
      "Epoch 65/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1071 - val_loss: 0.1099\n",
      "Epoch 66/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1058 - val_loss: 0.1098\n",
      "Epoch 67/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1084 - val_loss: 0.1092\n",
      "Epoch 68/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1066 - val_loss: 0.1093\n",
      "Epoch 69/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1078 - val_loss: 0.1095\n",
      "Epoch 70/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1062 - val_loss: 0.1094\n",
      "Epoch 71/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1067 - val_loss: 0.1092\n",
      "Epoch 72/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1073 - val_loss: 0.1097\n",
      "Epoch 73/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1096\n",
      "Epoch 74/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1075 - val_loss: 0.1097\n",
      "Epoch 75/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1066 - val_loss: 0.1092\n",
      "Epoch 76/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1074 - val_loss: 0.1091\n",
      "Epoch 77/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1099\n",
      "Epoch 78/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1082 - val_loss: 0.1100\n",
      "Epoch 79/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1077 - val_loss: 0.1096\n",
      "Epoch 80/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1055 - val_loss: 0.1090\n",
      "Epoch 81/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1068 - val_loss: 0.1089\n",
      "Epoch 82/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1075 - val_loss: 0.1097\n",
      "Epoch 83/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1074 - val_loss: 0.1092\n",
      "Epoch 84/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1100\n",
      "Epoch 85/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1090\n",
      "Epoch 86/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1071 - val_loss: 0.1087\n",
      "Epoch 87/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1096\n",
      "Epoch 88/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1068 - val_loss: 0.1093\n",
      "Epoch 89/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1066 - val_loss: 0.1090\n",
      "Epoch 90/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1072 - val_loss: 0.1097\n",
      "Epoch 91/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1083 - val_loss: 0.1095\n",
      "Epoch 92/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1073 - val_loss: 0.1088\n",
      "Epoch 93/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1075 - val_loss: 0.1095\n",
      "Epoch 94/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1051 - val_loss: 0.1088\n",
      "Epoch 95/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1067 - val_loss: 0.1085\n",
      "Epoch 96/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1095\n",
      "Epoch 97/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1064 - val_loss: 0.1088\n",
      "Epoch 98/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1063 - val_loss: 0.1089\n",
      "Epoch 99/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1062 - val_loss: 0.1091\n",
      "Epoch 100/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1064 - val_loss: 0.1089\n",
      "Epoch 101/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1069 - val_loss: 0.1099\n",
      "Epoch 102/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1092\n",
      "Epoch 103/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1092\n",
      "Epoch 104/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1072 - val_loss: 0.1089\n",
      "Epoch 105/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1068 - val_loss: 0.1087\n",
      "Epoch 106/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1062 - val_loss: 0.1092\n",
      "Epoch 107/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1057 - val_loss: 0.1086\n",
      "Epoch 108/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1070 - val_loss: 0.1094\n",
      "Epoch 109/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1086\n",
      "Epoch 110/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1058 - val_loss: 0.1089\n",
      "Epoch 111/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1062 - val_loss: 0.1083\n",
      "Epoch 112/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1069 - val_loss: 0.1092\n",
      "Epoch 113/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1069 - val_loss: 0.1102\n",
      "Epoch 114/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1075 - val_loss: 0.1098\n",
      "Epoch 115/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1065 - val_loss: 0.1096\n",
      "Epoch 116/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1093\n",
      "Epoch 117/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1062 - val_loss: 0.1093\n",
      "Epoch 118/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1063 - val_loss: 0.1085\n",
      "Epoch 119/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1058 - val_loss: 0.1091\n",
      "Epoch 120/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1095\n",
      "Epoch 121/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1091\n",
      "Epoch 122/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1055 - val_loss: 0.1101\n",
      "Epoch 123/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1053 - val_loss: 0.1101\n",
      "Epoch 124/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1058 - val_loss: 0.1093\n",
      "Epoch 125/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1062 - val_loss: 0.1101\n",
      "Epoch 126/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1046 - val_loss: 0.1098\n",
      "Epoch 127/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1063 - val_loss: 0.1101\n",
      "Epoch 128/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1053 - val_loss: 0.1100\n",
      "Epoch 129/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1049 - val_loss: 0.1102\n",
      "Epoch 130/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1047 - val_loss: 0.1105\n",
      "Epoch 131/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1046 - val_loss: 0.1100\n",
      "Epoch 132/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1051 - val_loss: 0.1099\n",
      "Epoch 133/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1040 - val_loss: 0.1111\n",
      "Epoch 134/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1044 - val_loss: 0.1098\n",
      "Epoch 135/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1049 - val_loss: 0.1103\n",
      "Epoch 136/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1056 - val_loss: 0.1107\n",
      "Epoch 137/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1055 - val_loss: 0.1103\n",
      "Epoch 138/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1064 - val_loss: 0.1100\n",
      "Epoch 139/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1060 - val_loss: 0.1105\n",
      "Epoch 140/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1038 - val_loss: 0.1104\n",
      "Epoch 141/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1043 - val_loss: 0.1109\n",
      "Epoch 142/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1034 - val_loss: 0.1101\n",
      "Epoch 143/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1055 - val_loss: 0.1103\n",
      "Epoch 144/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1058 - val_loss: 0.1107\n",
      "Epoch 145/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1043 - val_loss: 0.1104\n",
      "Epoch 146/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1047 - val_loss: 0.1104\n",
      "Epoch 147/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1047 - val_loss: 0.1108\n",
      "Epoch 148/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1049 - val_loss: 0.1092\n",
      "Epoch 149/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1049 - val_loss: 0.1098\n",
      "Epoch 150/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1033 - val_loss: 0.1100\n",
      "Epoch 151/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1047 - val_loss: 0.1105\n",
      "Epoch 152/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1051 - val_loss: 0.1089\n",
      "Epoch 153/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1043 - val_loss: 0.1119\n",
      "Epoch 154/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1040 - val_loss: 0.1096\n",
      "Epoch 155/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1039 - val_loss: 0.1106\n",
      "Epoch 156/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1044 - val_loss: 0.1115\n",
      "Epoch 157/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1052 - val_loss: 0.1097\n",
      "Epoch 158/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1039 - val_loss: 0.1114\n",
      "Epoch 159/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1053 - val_loss: 0.1118\n",
      "Epoch 160/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1041 - val_loss: 0.1109\n",
      "Epoch 161/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1045 - val_loss: 0.1109\n",
      "Epoch 162/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1048 - val_loss: 0.1112\n",
      "Epoch 163/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1036 - val_loss: 0.1117\n",
      "Epoch 164/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1051 - val_loss: 0.1098\n",
      "Epoch 165/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1046 - val_loss: 0.1098\n",
      "Epoch 166/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1038 - val_loss: 0.1110\n",
      "Epoch 167/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1041 - val_loss: 0.1109\n",
      "Epoch 168/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1048 - val_loss: 0.1112\n",
      "Epoch 169/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1045 - val_loss: 0.1113\n",
      "Epoch 170/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1045 - val_loss: 0.1112\n",
      "Epoch 171/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1037 - val_loss: 0.1099\n",
      "Epoch 172/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1043 - val_loss: 0.1115\n",
      "Epoch 173/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1038 - val_loss: 0.1108\n",
      "Epoch 174/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1035 - val_loss: 0.1114\n",
      "Epoch 175/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1046 - val_loss: 0.1113\n",
      "Epoch 176/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1035 - val_loss: 0.1113\n",
      "Epoch 177/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1045 - val_loss: 0.1120\n",
      "Epoch 178/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1039 - val_loss: 0.1104\n",
      "Epoch 179/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1036 - val_loss: 0.1126\n",
      "Epoch 180/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1045 - val_loss: 0.1107\n",
      "Epoch 181/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1031 - val_loss: 0.1105\n",
      "Epoch 182/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1030 - val_loss: 0.1111\n",
      "Epoch 183/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1034 - val_loss: 0.1101\n",
      "Epoch 184/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1038 - val_loss: 0.1123\n",
      "Epoch 185/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1034 - val_loss: 0.1131\n",
      "Epoch 186/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1041 - val_loss: 0.1109\n",
      "Epoch 187/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1046 - val_loss: 0.1117\n",
      "Epoch 188/2000\n",
      "35/35 - 0s - 3ms/step - loss: 0.1033 - val_loss: 0.1129\n",
      "Epoch 189/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1029 - val_loss: 0.1112\n",
      "Epoch 190/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1032 - val_loss: 0.1114\n",
      "Epoch 191/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1026 - val_loss: 0.1115\n",
      "Epoch 192/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1040 - val_loss: 0.1116\n",
      "Epoch 193/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1042 - val_loss: 0.1114\n",
      "Epoch 194/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1039 - val_loss: 0.1122\n",
      "Epoch 195/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1040 - val_loss: 0.1113\n",
      "Epoch 196/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1023 - val_loss: 0.1103\n",
      "Epoch 197/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1032 - val_loss: 0.1122\n",
      "Epoch 198/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1029 - val_loss: 0.1097\n",
      "Epoch 199/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1030 - val_loss: 0.1131\n",
      "Epoch 200/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1022 - val_loss: 0.1120\n",
      "Epoch 201/2000\n",
      "35/35 - 0s - 4ms/step - loss: 0.1034 - val_loss: 0.1122\n",
      "Epoch 202/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1032 - val_loss: 0.1125\n",
      "Epoch 203/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1028 - val_loss: 0.1118\n",
      "Epoch 204/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1020 - val_loss: 0.1127\n",
      "Epoch 205/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1033 - val_loss: 0.1119\n",
      "Epoch 206/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1040 - val_loss: 0.1124\n",
      "Epoch 207/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1021 - val_loss: 0.1108\n",
      "Epoch 208/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1032 - val_loss: 0.1133\n",
      "Epoch 209/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1026 - val_loss: 0.1119\n",
      "Epoch 210/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1021 - val_loss: 0.1119\n",
      "Epoch 211/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1023 - val_loss: 0.1130\n",
      "Epoch 212/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1024 - val_loss: 0.1128\n",
      "Epoch 213/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1023 - val_loss: 0.1132\n",
      "Epoch 214/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1010 - val_loss: 0.1125\n",
      "Epoch 215/2000\n",
      "35/35 - 0s - 2ms/step - loss: 0.1031 - val_loss: 0.1126\n",
      "Epoch 216/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      7\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,)),\n\u001b[1;32m      8\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m   layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Output layer with 2 neurons\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs_Y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrs_X_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs_Y_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:324\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m--> 324\u001b[0m         \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    326\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    327\u001b[0m             step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    328\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:98\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m     96\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, logs)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(10, activation='relu', input_shape=(2,)),\n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(10, activation='relu', input_shape=(2,)),  # Hidden layer\n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(2)  # Output layer with 2 neurons\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(rs_X_train, rs_Y_train, epochs=2000, validation_data=(rs_X_val, rs_Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatically testing different network layers\n",
    "\n",
    "Scaling the data has helped dramatically, but it would help to know what kind of layers would be useful. This is a little slow but will be a good way to empirically measure the loss for different functions. Just starting with two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer Combination 1 out of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmy/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer Combination 2 out of 16\n",
      "Training Layer Combination 3 out of 16\n",
      "Training Layer Combination 4 out of 16\n",
      "Training Layer Combination 5 out of 16\n",
      "Training Layer Combination 6 out of 16\n",
      "Training Layer Combination 7 out of 16\n",
      "Training Layer Combination 8 out of 16\n",
      "Training Layer Combination 9 out of 16\n",
      "Training Layer Combination 10 out of 16\n",
      "Training Layer Combination 11 out of 16\n",
      "Training Layer Combination 12 out of 16\n",
      "Training Layer Combination 13 out of 16\n",
      "Training Layer Combination 14 out of 16\n",
      "Training Layer Combination 15 out of 16\n",
      "Training Layer Combination 16 out of 16\n"
     ]
    }
   ],
   "source": [
    "act_funcs = ['leaky_relu','relu','sigmoid','tanh']\n",
    "losses = []\n",
    "layernum = 0\n",
    "\n",
    "for i in range(len(act_funcs)):\n",
    "    for j in range (len(act_funcs)):\n",
    "        layernum += 1\n",
    "        print(\"Training Layer Combination \" + str(layernum) + \" out of \" + str((len(act_funcs))**2))\n",
    "        model = tf.keras.Sequential([\n",
    "        layers.Dense(2),\n",
    "        layers.Dense(10, activation=act_funcs[i], input_shape=(2,)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(10, activation='relu', input_shape=(2,)),  # Hidden layer\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(2)  # Output layer with 2 neurons\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        history = model.fit(rs_X_train, rs_Y_train, epochs=300, validation_data=(rs_X_val, rs_Y_val), verbose = 0)\n",
    "        final_epoch_loss = history.history['loss'][-1]\n",
    "        losses.append({'Layer 1' : act_funcs[i], 'Layer 2': act_funcs[j], 'Loss': final_epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer 1</th>\n",
       "      <th>Layer 2</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Layer 1 Neurons</th>\n",
       "      <th>Layer 2 Neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>0.087611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.091828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.089810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.089719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>0.091886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.092415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.094143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.086029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>0.104498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.104680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.103908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tanh</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.098942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.098984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanh</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.094461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109989</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111456</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105124</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106503</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103544</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108520</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111457</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.102972</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104027</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103255</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104747</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Layer 1     Layer 2      Loss  Layer 1 Neurons  Layer 2 Neurons\n",
       "0   leaky_relu  leaky_relu  0.087611              NaN              NaN\n",
       "1   leaky_relu        relu  0.091828              NaN              NaN\n",
       "2   leaky_relu     sigmoid  0.089810              NaN              NaN\n",
       "3   leaky_relu        tanh  0.089719              NaN              NaN\n",
       "4         relu  leaky_relu  0.091886              NaN              NaN\n",
       "5         relu        relu  0.092415              NaN              NaN\n",
       "6         relu     sigmoid  0.094143              NaN              NaN\n",
       "7         relu        tanh  0.086029              NaN              NaN\n",
       "8      sigmoid  leaky_relu  0.104498              NaN              NaN\n",
       "9      sigmoid        relu  0.104680              NaN              NaN\n",
       "10     sigmoid     sigmoid  0.103908              NaN              NaN\n",
       "11     sigmoid        tanh  0.104403              NaN              NaN\n",
       "12        tanh  leaky_relu  0.097530              NaN              NaN\n",
       "13        tanh        relu  0.098942              NaN              NaN\n",
       "14        tanh     sigmoid  0.098984              NaN              NaN\n",
       "15        tanh        tanh  0.094461              NaN              NaN\n",
       "16         NaN         NaN  0.106005              1.0              1.0\n",
       "17         NaN         NaN  0.111464              1.0              2.0\n",
       "18         NaN         NaN  0.111443              1.0              3.0\n",
       "19         NaN         NaN  0.111533              1.0              4.0\n",
       "20         NaN         NaN  0.109989              2.0              1.0\n",
       "21         NaN         NaN  0.111456              2.0              2.0\n",
       "22         NaN         NaN  0.105124              2.0              3.0\n",
       "23         NaN         NaN  0.106503              2.0              4.0\n",
       "24         NaN         NaN  0.103544              3.0              1.0\n",
       "25         NaN         NaN  0.108520              3.0              2.0\n",
       "26         NaN         NaN  0.111457              3.0              3.0\n",
       "27         NaN         NaN  0.107700              3.0              4.0\n",
       "28         NaN         NaN  0.102972              4.0              1.0\n",
       "29         NaN         NaN  0.104027              4.0              2.0\n",
       "30         NaN         NaN  0.103255              4.0              3.0\n",
       "31         NaN         NaN  0.104747              4.0              4.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossframe = pd.DataFrame(losses)\n",
    "lossframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now it'slooking like relu -> tanh may be the way to go. Now testing number of neurons. Also basing sequential layers on initial layer size as a 'filtering' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer Combination 1 out of -14\n",
      "Training Layer Combination 2 out of 2\n",
      "Training Layer Combination 3 out of 34\n",
      "Training Layer Combination 4 out of 98\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "max_neurons = 64\n",
    "min_neurons = 30\n",
    "layernum = 0\n",
    "\n",
    "for i in range(4,8):\n",
    "    max_neurons = 2**i\n",
    "    layernum += 1\n",
    "    print(\"Training Layer Combination \" + str(layernum) + \" out of \" + str(max_neurons - min_neurons))\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Dense(2),\n",
    "    layers.Dense(max_neurons, activation='relu', input_shape=(2,)),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(int(max_neurons/2), activation='tanh', input_shape=(2,)), \n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(int(max_neurons/4), activation='relu', input_shape=(2,)),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(2) \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    history = model.fit(rs_X_train, rs_Y_train, epochs=300, validation_data=(rs_X_val, rs_Y_val), verbose = 0)\n",
    "    final_epoch_loss = history.history['loss'][-1]\n",
    "    losses.append({'Layer 1 Neurons' : i, 'Layer 2 Neurons': str(i/2),'Layer 3 Neurons': str(i/4), 'Loss': final_epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer 1 Neurons</th>\n",
       "      <th>Layer 2 Neurons</th>\n",
       "      <th>Layer 3 Neurons</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.094603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.084221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.078768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.074874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer 1 Neurons Layer 2 Neurons Layer 3 Neurons      Loss\n",
       "0                4             2.0             1.0  0.094603\n",
       "1                5             2.5            1.25  0.084221\n",
       "2                6             3.0             1.5  0.078768\n",
       "3                7             3.5            1.75  0.074874"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossframe = pd.DataFrame(losses)\n",
    "lossframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(953, 1, 2), dtype=float64, numpy=\n",
       "array([[[1.        , 0.9226546 ]],\n",
       "\n",
       "       [[1.        , 0.92469183]],\n",
       "\n",
       "       [[1.        , 0.92565864]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.43267108, 0.95759815]],\n",
       "\n",
       "       [[0.43267108, 0.95773627]],\n",
       "\n",
       "       [[0.43267108, 0.95797797]]])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_X_train_shape = tf.reshape(rs_X_train, [len(rs_X_train),1, 2])\n",
    "rs_Y_train_shape = tf.reshape(rs_Y_train, [len(rs_Y_train),1, 2])\n",
    "rs_X_val_shape = tf.reshape(rs_X_val, [len(rs_X_val),1, 2])\n",
    "rs_Y_val_shape = tf.reshape(rs_Y_val, [len(rs_Y_val),1, 2])\n",
    "rs_X_train_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 20:15:24.023063: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 67108864 exceeds 10% of free system memory.\n",
      "2024-03-29 20:15:24.059737: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 67108864 exceeds 10% of free system memory.\n",
      "2024-03-29 20:15:24.096446: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 67108864 exceeds 10% of free system memory.\n",
      "2024-03-29 20:15:24.136460: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 67108864 exceeds 10% of free system memory.\n",
      "2024-03-29 20:15:24.172665: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 67108864 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 3s - 112ms/step - loss: 0.1266 - val_loss: 0.1576\n",
      "Epoch 2/2000\n",
      "30/30 - 1s - 39ms/step - loss: 0.1089 - val_loss: 0.1699\n",
      "Epoch 3/2000\n",
      "30/30 - 1s - 40ms/step - loss: 0.1063 - val_loss: 0.1887\n",
      "Epoch 4/2000\n",
      "30/30 - 1s - 40ms/step - loss: 0.1005 - val_loss: 0.2246\n",
      "Epoch 5/2000\n",
      "30/30 - 1s - 40ms/step - loss: 0.0971 - val_loss: 0.1967\n",
      "Epoch 6/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      4\u001b[0m         layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      5\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv1D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     13\u001b[0m         ])\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs_X_train_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs_Y_train_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrs_X_val_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs_Y_val_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_neurons = 2**10\n",
    "drop_time = 0.2\n",
    "model = tf.keras.Sequential([\n",
    "        layers.Dense(2),\n",
    "        layers.Conv1D(filters=32, kernel_size=1, activation='relu', input_shape=(1,2)),\n",
    "        layers.Dense(max_neurons, activation='relu', input_shape=(2,)),\n",
    "        layers.Dropout(drop_time),\n",
    "        layers.Dense(int(max_neurons/2), activation='tanh', input_shape=(2,)), \n",
    "        layers.Dropout(drop_time),\n",
    "        layers.Dense(int(max_neurons/4), activation='relu', input_shape=(2,)),\n",
    "        layers.Dropout(drop_time),\n",
    "        layers.Dense(2) \n",
    "        ])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(rs_X_train_shape, rs_Y_train_shape, epochs=2000, validation_data=(rs_X_val_shape, rs_Y_val_shape), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to now undo scaling for predictions to actually know what's happening. This will *definitely* just be a function in lcvr_learning when this is all implemented there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "tf.Tensor([[0.33869303 3.15758747]], shape=(1, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "pred_wave = [[480]]\n",
    "pred_volts =[[0.03]]\n",
    "\n",
    "rs_input = tf.concat([\n",
    "    tf.convert_to_tensor(scaler.fit(X_train[:,0:1]).transform(pred_wave)),\n",
    "    tf.convert_to_tensor(scaler.fit(X_train[:,1:]).transform(pred_volts))\n",
    "], axis=1)\n",
    "\n",
    "prediction = model.predict(rs_input)\n",
    "\n",
    "rs_output = tf.concat([\n",
    "    tf.convert_to_tensor(scaler.fit(Y_train[:,0:1]).inverse_transform([[prediction[0,0]]])),\n",
    "    tf.convert_to_tensor(scaler.fit(Y_train[:,1:]).inverse_transform([[prediction[0,1]]]))\n",
    "], axis=1)\n",
    "print(rs_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.321365]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(scaler.fit(Y_train[:,0:1]).inverse_transform([[.0687]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(953, 2), dtype=float64, numpy=\n",
       "array([[1.        , 0.9226546 ],\n",
       "       [1.        , 0.92469183],\n",
       "       [1.        , 0.92565864],\n",
       "       ...,\n",
       "       [0.43267108, 0.95759815],\n",
       "       [0.43267108, 0.95773627],\n",
       "       [0.43267108, 0.95797797]])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
